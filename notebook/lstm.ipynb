{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 官方API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.0193e-01,  4.7390e-01, -2.5187e-01, -4.2785e-03, -1.7096e-01],\n",
      "         [-1.0270e-01,  7.0782e-02, -4.1513e-02, -1.4709e-01,  2.0677e-01],\n",
      "         [-1.8238e-01,  1.3335e-01,  3.8524e-02, -1.3338e-01,  3.5900e-01]],\n",
      "\n",
      "        [[-5.8291e-01,  4.3477e-01, -3.3951e-01, -3.2018e-01,  1.3939e-01],\n",
      "         [-1.8615e-01,  2.1970e-01, -1.1651e-01, -4.0408e-01, -4.7944e-04],\n",
      "         [-2.4394e-01,  2.5711e-01, -1.3780e-01, -4.6653e-02,  1.1308e-01]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "weight_ih_l0 torch.Size([20, 4])\n",
      "weight_hh_l0 torch.Size([20, 5])\n",
      "bias_ih_l0 torch.Size([20])\n",
      "bias_hh_l0 torch.Size([20])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nweight_ih_l0 torch.Size([20, 4]) 这里的20是5*4,4是因为w要和input进行相乘，所以是input_size\\nweight_hh_l0 torch.Size([20, 5]) 4是因为w要和隐含状态进行相乘，所以是hidden_size \\nbias_ih_l0 torch.Size([20]) 这里的20是4*5， 4个bias\\nbias_hh_l0 torch.Size([20])\\n\\n'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# input_size 输入序列的特征大小, seq_len：时间  hidden_size 网络的细胞状态的大小\n",
    "batch_size, seq_len, input_size,hidden_size = 2,3,4,5\n",
    "input = torch.randn(batch_size, seq_len, input_size)\n",
    "c0 = torch.randn(batch_size,hidden_size) # 初始值，随机得到，不需要训练\n",
    "h0 = torch.randn(batch_size,hidden_size) # \n",
    "\n",
    "\n",
    "# 调用官方LSTM API\n",
    "lstm_layer = nn.LSTM(input_size, hidden_size,batch_first=True)\n",
    "output,(h_final,c_final) = lstm_layer(input,(h0.unsqueeze(0),c0.unsqueeze(0))) # 扩充一维，变成三维\n",
    "\n",
    "print(output)\n",
    "\n",
    "# 可以看lstm网络有哪些权重，有哪些张量\n",
    "for p,name in lstm_layer.named_parameters():  \n",
    "    # print(p,name)\n",
    "    print(p,name.shape)\n",
    "\n",
    "\"\"\"\n",
    "weight_ih_l0 torch.Size([20, 4]) 这里的20是5*4,4是因为w要和input进行相乘，所以是input_size\n",
    "weight_hh_l0 torch.Size([20, 5]) 4是因为w要和隐含状态进行相乘，所以是hidden_size \n",
    "bias_ih_l0 torch.Size([20]) 这里的20是4*5， 4个bias\n",
    "bias_hh_l0 torch.Size([20])\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 源码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1019,  0.4231, -0.2686, -0.0042, -0.1567],\n",
       "         [-0.1010,  0.0623, -0.0429, -0.1449,  0.1853],\n",
       "         [-0.1797,  0.1242,  0.0420, -0.1281,  0.3195]],\n",
       "\n",
       "        [[-0.5829,  0.3668, -0.3588, -0.3171,  0.1132],\n",
       "         [-0.1831,  0.1788, -0.1254, -0.4000, -0.0014],\n",
       "         [-0.2320,  0.2242, -0.1408, -0.0470,  0.0979]]], grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def lstm_forward(input,initial_states, w_ih,w_hh,b_ih,b_hh):\n",
    "    h0,c0 = initial_states \n",
    "    batch_size, seq_len, input_size = input.shape\n",
    "    hidden_size = w_ih.shape[0]//4\n",
    "\n",
    "    prev_h = h0\n",
    "    prev_c = c0  # c_t\n",
    "\n",
    "    output_size = hidden_size\n",
    "    output = torch.zeros(batch_size, seq_len,output_size) # 初始化输出序列\n",
    "\n",
    "    # w_ih的维度(4*hidden_size,input_size),w_hh的维度(4*hidden_size,hidden_size)，需要扩维，把batch_size扩充出来\n",
    "    batch_w_ih = w_ih.unsqueeze(0).tile(batch_size,1,1) # 先扩充0维，再对第0维复制batch_size遍,此时w_ih的维度(batch_size, 4*hidden_size,input_size)\n",
    "    batch_w_hh = w_hh.unsqueeze(0).tile(batch_size,1,1) # 此时w_hh的维度(batch_size, 4*hidden_size,hidden_size)\n",
    "\n",
    "\n",
    "\n",
    "    for t in range(seq_len):\n",
    "        x = input[:,t,:]  # 当前时刻的输入向量 (batch_size, input_size)\n",
    "        # 带batch的矩阵相乘 bmm\n",
    "        w_times_x = torch.bmm(batch_w_ih, x.unsqueeze(-1)) # (batch_size, 4*hidden_size, 1)\n",
    "        w_times_x = w_times_x.squeeze(-1) # 把最后一维去掉 (batch_size, 4*hidden_size)\n",
    "\n",
    "        w_times_h_prev = torch.bmm(batch_w_hh, prev_h.unsqueeze(-1)) \n",
    "        w_times_h_prev = w_times_h_prev.squeeze(-1)  #  (batch_size, 4*hidden_size)\n",
    "\n",
    "        # 分别计算输入门(i)、遗忘门(f)、cell(g)、输出门(o)\n",
    "        #  w_times_x[:,:hidden_size]取前四分之一 遗忘门取四分之一到二分之一\n",
    "        i_t = torch.sigmoid(w_times_x[:,:hidden_size] + w_times_h_prev[:,:hidden_size] + b_ih[:hidden_size] + b_hh[:hidden_size]) \n",
    "        f_t = torch.sigmoid(w_times_x[:,hidden_size:hidden_size*2] + w_times_h_prev[:,hidden_size:hidden_size*2] + b_ih[hidden_size:2*hidden_size] + b_hh[hidden_size:2*hidden_size]) \n",
    "        g_t = torch.tanh(w_times_x[:,2*hidden_size:hidden_size*3] + w_times_h_prev[:,2*hidden_size:hidden_size*3] + b_ih[hidden_size*2:3*hidden_size] + b_hh[hidden_size*2:3*hidden_size]) \n",
    "        o_t = torch.sigmoid(w_times_x[:,3*hidden_size:hidden_size*4] + w_times_h_prev[:,3*hidden_size:hidden_size*4] + b_ih[hidden_size*3:4*hidden_size] + b_hh[hidden_size*3]) \n",
    "   \n",
    "        prev_c = f_t * prev_c + i_t * g_t\n",
    "        prev_h = o_t * torch.tanh(prev_c)\n",
    "\n",
    "        output[:,t,:] = prev_h\n",
    "\n",
    "    return output, (prev_h, prev_c)\n",
    "\n",
    "\n",
    "custom_output, (h_final_custom, c_final_custom) = lstm_forward(input, (h0,c0), \n",
    "             lstm_layer.weight_ih_l0,lstm_layer.weight_hh_l0,\n",
    "             lstm_layer.bias_ih_l0, lstm_layer.bias_hh_l0)\n",
    "\n",
    "custom_output\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Projection\n",
    "\n",
    "对hidden_size进行压缩"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 官方API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.3685, -0.1899,  0.0366],\n",
      "         [ 0.2518, -0.1133, -0.0095],\n",
      "         [ 0.1749, -0.1180, -0.0171]],\n",
      "\n",
      "        [[-0.1184, -0.1185,  0.0494],\n",
      "         [-0.0045, -0.1582, -0.0012],\n",
      "         [-0.0664, -0.1176, -0.0316]]], grad_fn=<TransposeBackward0>)\n",
      "torch.Size([2, 3, 3])\n",
      "weight_ih_l0 torch.Size([20, 4])\n",
      "weight_hh_l0 torch.Size([20, 3])\n",
      "bias_ih_l0 torch.Size([20])\n",
      "bias_hh_l0 torch.Size([20])\n",
      "weight_hr_l0 torch.Size([3, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nweight_ih_l0 torch.Size([20, 4]) 这里的20是5*4,4是因为w要和input进行相乘，所以是input_size\\nweight_hh_l0 torch.Size([20, 5]) 4是因为w要和隐含状态进行相乘，所以是hidden_size \\nbias_ih_l0 torch.Size([20]) 这里的20是4*5， 4个bias\\nbias_hh_l0 torch.Size([20])\\nweight_hr_l0 torch.Size([3, 5])  对Hiddenstate进行压缩的参数\\n'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# input_size 输入序列的特征大小, seq_len：时间  hidden_size 网络的细胞状态的大小\n",
    "batch_size, seq_len, input_size,hidden_size = 2,3,4,5\n",
    "proj_size = 3\n",
    "input = torch.randn(batch_size, seq_len, input_size)\n",
    "c0 = torch.randn(batch_size,hidden_size) # 初始值，随机得到，不需要训练\n",
    "h0 = torch.randn(batch_size,proj_size) # 改\n",
    "\n",
    "\n",
    "\n",
    "# 调用官方LSTM API\n",
    "lstm_layer_p = nn.LSTM(input_size, hidden_size,batch_first=True, proj_size=proj_size)\n",
    "output,(h_final,c_final) = lstm_layer_p(input,(h0.unsqueeze(0),c0.unsqueeze(0))) # 扩充一维，变成三维\n",
    "\n",
    "print(output)\n",
    "print(output.shape)\n",
    "\n",
    "# 可以看lstm网络有哪些权重，有哪些张量\n",
    "for p,name in lstm_layer_p.named_parameters():  \n",
    "    # print(p,name)\n",
    "    print(p,name.shape)\n",
    "\n",
    "\"\"\"\n",
    "weight_ih_l0 torch.Size([20, 4]) 这里的20是5*4,4是因为w要和input进行相乘，所以是input_size\n",
    "weight_hh_l0 torch.Size([20, 5]) 4是因为w要和隐含状态进行相乘，所以是hidden_size \n",
    "bias_ih_l0 torch.Size([20]) 这里的20是4*5， 4个bias\n",
    "bias_hh_l0 torch.Size([20])\n",
    "weight_hr_l0 torch.Size([3, 5])  对Hiddenstate进行压缩的参数\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### projection 源码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n",
      "torch.Size([2, 3])\n",
      "torch.Size([2, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3719, -0.2033,  0.0343],\n",
       "         [ 0.2523, -0.1192, -0.0108],\n",
       "         [ 0.1727, -0.1119, -0.0173]],\n",
       "\n",
       "        [[-0.0977, -0.1338,  0.0609],\n",
       "         [ 0.0043, -0.1583,  0.0062],\n",
       "         [-0.0611, -0.1174, -0.0264]]], grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lstm_forward_proj(input,initial_states, w_ih,w_hh,b_ih,b_hh, w_hr=None):\n",
    "    h0,c0 = initial_states \n",
    "    batch_size, seq_len, input_size = input.shape\n",
    "    hidden_size = w_ih.shape[0]//4\n",
    "\n",
    "    prev_h = h0\n",
    "    prev_c = c0  # c_t\n",
    "\n",
    "    # w_ih的维度(4*hidden_size,input_size),w_hh的维度(4*hidden_size,hidden_size)，需要扩维，把batch_size扩充出来\n",
    "    batch_w_ih = w_ih.unsqueeze(0).tile(batch_size,1,1) # 先扩充0维，再对第0维复制batch_size遍,此时w_ih的维度(batch_size, 4*hidden_size,input_size)\n",
    "    batch_w_hh = w_hh.unsqueeze(0).tile(batch_size,1,1) # 此时w_hh的维度(batch_size, 4*hidden_size,hidden_size)\n",
    "\n",
    "\n",
    "    if w_hr is not None:\n",
    "        proj_size, _ = w_hr.shape\n",
    "        output_size = proj_size\n",
    "        batch_w_hr = w_hr.unsqueeze(0).tile(batch_size,1,1)\n",
    "    else:\n",
    "        output_size = hidden_size\n",
    "\n",
    "    output = torch.zeros(batch_size, seq_len,output_size) # 初始化输出序列\n",
    "\n",
    "\n",
    "    for t in range(seq_len):\n",
    "        x = input[:,t,:]  # 当前时刻的输入向量 (batch_size, input_size)\n",
    "        # 带batch的矩阵相乘 bmm\n",
    "        w_times_x = torch.bmm(batch_w_ih, x.unsqueeze(-1)) # (batch_size, 4*hidden_size, 1)\n",
    "        w_times_x = w_times_x.squeeze(-1) # 把最后一维去掉 (batch_size, 4*hidden_size)\n",
    "\n",
    "        w_times_h_prev = torch.bmm(batch_w_hh, prev_h.unsqueeze(-1)) \n",
    "        w_times_h_prev = w_times_h_prev.squeeze(-1)  #  (batch_size, 4*hidden_size)\n",
    "\n",
    "        # 分别计算输入门(i)、遗忘门(f)、cell(g)、输出门(o)\n",
    "        #  w_times_x[:,:hidden_size]取前四分之一 遗忘门取四分之一到二分之一\n",
    "        i_t = torch.sigmoid(w_times_x[:,:hidden_size] + w_times_h_prev[:,:hidden_size] + b_ih[:hidden_size] + b_hh[:hidden_size]) \n",
    "        f_t = torch.sigmoid(w_times_x[:,hidden_size:hidden_size*2] + w_times_h_prev[:,hidden_size:hidden_size*2] + b_ih[hidden_size:2*hidden_size] + b_hh[hidden_size:2*hidden_size]) \n",
    "        g_t = torch.tanh(w_times_x[:,2*hidden_size:hidden_size*3] + w_times_h_prev[:,2*hidden_size:hidden_size*3] + b_ih[hidden_size*2:3*hidden_size] + b_hh[hidden_size*2:3*hidden_size]) \n",
    "        o_t = torch.sigmoid(w_times_x[:,3*hidden_size:hidden_size*4] + w_times_h_prev[:,3*hidden_size:hidden_size*4] + b_ih[hidden_size*3:4*hidden_size] + b_hh[hidden_size*3]) \n",
    "   \n",
    "        prev_c = f_t * prev_c + i_t * g_t\n",
    "        prev_h = o_t * torch.tanh(prev_c) #  [batch_size, hidden_size]\n",
    "\n",
    "        if w_hr is not None:  # 对prev_h进行压缩\n",
    "            prev_h = torch.bmm(batch_w_hr, prev_h.unsqueeze(-1))\n",
    "            prev_h = prev_h.squeeze(-1) # [batch_size, proj_sieze ]\n",
    "            # print(prev_h.shape)\n",
    "\n",
    "        output[:,t,:] = prev_h\n",
    "\n",
    "    return output, (prev_h, prev_c)\n",
    "\n",
    "\n",
    "custom_output, (h_final_custom, c_final_custom) = lstm_forward_proj(input, (h0,c0), \n",
    "             lstm_layer_p.weight_ih_l0,lstm_layer_p.weight_hh_l0,\n",
    "             lstm_layer_p.bias_ih_l0, lstm_layer_p.bias_hh_l0,\n",
    "             lstm_layer_p.weight_hr_l0)\n",
    "\n",
    "custom_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
