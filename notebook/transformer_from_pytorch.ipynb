{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer难点理解与实现\n",
    "\n",
    "### Transformer细节实现的难点\n",
    "- word embedding\n",
    "- position embedding\n",
    "- mask\n",
    "    - encoder self-attention mask\n",
    "    - intra-attention mask\n",
    "    - decoder self-attention mask\n",
    "- multi-head self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. word embedding\n",
    "在实际任务中，怎么构建word embedding？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src_len:\n",
      "tensor([3, 4], dtype=torch.int32)\n",
      " tgt_len:\n",
      "tensor([3, 2], dtype=torch.int32)\n",
      "src_seq:\n",
      "[tensor([1, 7, 3]), tensor([5, 6, 1, 7])]\n",
      " tgt_seq:\n",
      "[tensor([7, 1, 7]), tensor([7, 3])]\n",
      "src_seq_padded:\n",
      "[tensor([1, 7, 3, 0, 0]), tensor([5, 6, 1, 7, 0])] \n",
      "tgt_seq_padded:\n",
      "[tensor([7, 1, 7, 0, 0]), tensor([7, 3, 0, 0, 0])]\n",
      "src_seq_cat:\n",
      "tensor([[1, 7, 3, 0, 0],\n",
      "        [5, 6, 1, 7, 0]]) \n",
      " tgt_seq_cat:\n",
      "tensor([[7, 1, 7, 0, 0],\n",
      "        [7, 3, 0, 0, 0]])\n",
      "Parameter containing:\n",
      "tensor([[ 0.2615,  0.9311, -0.5145, -1.6517,  1.0460,  0.5222, -0.1668,  0.0530],\n",
      "        [ 0.5638,  2.2566,  1.8693, -1.1952,  0.9979,  0.4592,  2.4364, -0.1468],\n",
      "        [-0.4760, -0.2929, -0.3481,  0.3487,  0.0371, -0.0677,  0.4290, -0.8681],\n",
      "        [-0.2712,  0.1416,  0.1295,  0.6814, -0.9583,  0.0639,  0.6589,  0.8195],\n",
      "        [-0.4554,  2.2124, -0.3770, -0.1437,  0.6480, -2.3256,  1.2683, -0.2483],\n",
      "        [ 0.9578, -1.2890, -1.6483,  0.8290, -0.8373, -0.5296,  1.3544,  1.3778],\n",
      "        [-0.0752, -0.4233,  0.4217, -0.2576, -1.5835,  1.3960, -1.0319,  1.1391],\n",
      "        [ 0.3174,  0.1450, -0.6232, -0.0112, -0.3531, -0.8388, -1.2329, -0.6806],\n",
      "        [ 0.5161, -0.8823,  0.5617,  1.4444, -0.2090, -3.1117, -0.4260,  1.3343]],\n",
      "       requires_grad=True)\n",
      "tensor([[[ 0.5638,  2.2566,  1.8693, -1.1952,  0.9979,  0.4592,  2.4364,\n",
      "          -0.1468],\n",
      "         [ 0.3174,  0.1450, -0.6232, -0.0112, -0.3531, -0.8388, -1.2329,\n",
      "          -0.6806],\n",
      "         [-0.2712,  0.1416,  0.1295,  0.6814, -0.9583,  0.0639,  0.6589,\n",
      "           0.8195],\n",
      "         [ 0.2615,  0.9311, -0.5145, -1.6517,  1.0460,  0.5222, -0.1668,\n",
      "           0.0530],\n",
      "         [ 0.2615,  0.9311, -0.5145, -1.6517,  1.0460,  0.5222, -0.1668,\n",
      "           0.0530]],\n",
      "\n",
      "        [[ 0.9578, -1.2890, -1.6483,  0.8290, -0.8373, -0.5296,  1.3544,\n",
      "           1.3778],\n",
      "         [-0.0752, -0.4233,  0.4217, -0.2576, -1.5835,  1.3960, -1.0319,\n",
      "           1.1391],\n",
      "         [ 0.5638,  2.2566,  1.8693, -1.1952,  0.9979,  0.4592,  2.4364,\n",
      "          -0.1468],\n",
      "         [ 0.3174,  0.1450, -0.6232, -0.0112, -0.3531, -0.8388, -1.2329,\n",
      "          -0.6806],\n",
      "         [ 0.2615,  0.9311, -0.5145, -1.6517,  1.0460,  0.5222, -0.1668,\n",
      "           0.0530]]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5g/yvmtk8tn46v8qzbx5nl_4vd40000gn/T/ipykernel_7941/1000849770.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  src_seq_tensor = [torch.unsqueeze(torch.tensor(seq),0) for seq in src_seq_padded]\n",
      "/var/folders/5g/yvmtk8tn46v8qzbx5nl_4vd40000gn/T/ipykernel_7941/1000849770.py:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tgt_seq_tensor = [torch.unsqueeze(torch.tensor(seq),0) for seq in tgt_seq_padded]\n"
     ]
    }
   ],
   "source": [
    "# 以序列建模为例,考虑source sequence和target sequence \n",
    "\n",
    "# 构建序列，序列的字符以词表的索引的形式\n",
    "\n",
    "# 设置随机种子\n",
    "torch.manual_seed(3)\n",
    "\n",
    "\n",
    "batch_size = 2\n",
    "max_num_src_words= max_num_tgt_words = 8 # 原序列和目标序列的单词数最大都是8\n",
    "\n",
    "max_src_seq_len = max_tgt_seq_len = 5 # 最大序列长度\n",
    "\n",
    "src_len = torch.randint(2,5,(batch_size,)).to(dtype=torch.int32) # 随机生成整形，第一个位置是最小值，第二个位置是最大值，第三个是tuple\n",
    "tgt_len = torch.randint(2,5,(batch_size,)).to(dtype=torch.int32) \n",
    "\n",
    "print(f\"src_len:\\n{src_len}\\n tgt_len:\\n{tgt_len}\")\n",
    "# tensor([3, 4]) tensor([3, 2])\n",
    "# 表示src有两个batch，第一个句子长度为3，第二个句子长度为4.\n",
    "#  tgt有两个batch，第一个句子长度为3，第二个句子长度为2\n",
    "\n",
    "\n",
    "# 根据长度生成原序列和目标序列，是单词索引构成的句子\n",
    "src_seq = [torch.randint(1,max_num_src_words,(L,)) for L in src_len] # 单词的索引最小为1，最大为8，size就是src_len的长度\n",
    "tgt_seq = [torch.randint(1,max_num_tgt_words,(L,)) for L in tgt_len]\n",
    "print(f\"src_seq:\\n{src_seq}\\n tgt_seq:\\n{tgt_seq}\")\n",
    "\n",
    "#src_seq, tgt_seq\n",
    "# ([tensor([1, 7, 3]), tensor([5, 6, 1, 7])],\n",
    "#  [tensor([7, 1, 7]), tensor([7, 3])])\n",
    "# 表示src第一个句子是1,7,3，第二个句子是5,6,1,7\n",
    "# tgt第一个句子是7,1,7,第二个句子是7, 3\n",
    "\n",
    "# 进行padding，pad到最大长度\n",
    "# src_seq = [F.pad(torch.randint(1,max_num_src_words,(L,)),(0,max_src_seq_len-L)) for L in src_len]\n",
    "# tgt_seq = [F.pad(torch.randint(1,max_num_tgt_words,(L,)),(0,max_tgt_seq_len-L)) for L in tgt_len]\n",
    "\n",
    "src_seq_padded = [F.pad(seq,(0,max_src_seq_len-len(seq))) for seq in src_seq]\n",
    "tgt_seq_padded = [F.pad(seq,(0,max_tgt_seq_len-len(seq))) for seq in tgt_seq]\n",
    "print(f\"src_seq_padded:\\n{src_seq_padded} \\ntgt_seq_padded:\\n{tgt_seq_padded}\")\n",
    "# [tensor([1, 7, 3, 0, 0]), tensor([5, 6, 1, 7, 0])]  包含两个一维张量（1D tensors）的列表。每个张量都有5个元素，所以每个张量的维度是 [5]\n",
    "\n",
    "\n",
    "# 把src_seq和tgt_seq从列表变成二维Tensor：(batch_size, max_seq_len)\n",
    "# unsqueez变成二维，加一个0维，用torch.cat 在第0维cat起来\n",
    "\n",
    "src_seq_tensor = [torch.unsqueeze(torch.tensor(seq),0) for seq in src_seq_padded]\n",
    "#[tensor([[1, 7, 3, 0, 0]]), tensor([[5, 6, 1, 7, 0]])] 包含两个二维张量（2D tensors）的列表。每个张量的形状（dimension）是 [1, 5]\n",
    "src_seq_cat = torch.cat(src_seq_tensor, dim=0)            \n",
    "# tensor([[1, 7, 3, 0, 0],[5, 6, 1, 7, 0]]) 二维张量\n",
    "\n",
    "tgt_seq_tensor = [torch.unsqueeze(torch.tensor(seq),0) for seq in tgt_seq_padded]\n",
    "tgt_seq_cat = torch.cat(tgt_seq_tensor, dim=0) \n",
    "\n",
    "print(f\"src_seq_cat:\\n{src_seq_cat} \\n tgt_seq_cat:\\n{tgt_seq_cat}\")\n",
    "\n",
    "\n",
    "# 构造embedding\n",
    "# 通过torch.nn.Embedding来构造embedding的表，根据索引就可以从这个表中取某一行\n",
    "# 参数：num_embeddings, 单词表大小，注意这里是 max_num_src_words+ 1，因为0是pad的索引，前面 有padding填充的是0\n",
    "#  model_dim，模型的特征大小，通常是512\n",
    "model_dim = 8\n",
    "src_embedding_table = nn.Embedding(max_num_src_words + 1 ,model_dim) # 随机初始化\n",
    "tgt_embedding_table = nn.Embedding(max_num_tgt_words + 1 ,model_dim)\n",
    "print(src_embedding_table.weight)\n",
    "# 根据词的索引来查表得到embedding,传单词的索引\n",
    "src_embedding = src_embedding_table(src_seq_cat)\n",
    "tgt_embedding = tgt_embedding_table(tgt_seq_cat)\n",
    "\n",
    "print(src_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 2. position embedding\n",
    "Transformer没有局部性假设，也没有有序性假设，所以需要一个位置信息。\n",
    "\n",
    "$$ PE(pos,2i)=sin(pos/10000^{2i/d_{model}}) $$\n",
    "$$ PE(pos,2i+1)=cos(pos/10000^{2i/d_{model}}) $$\n",
    "\n",
    "PE是一个二维矩阵，行数是训练的序列的最大长度。每一列是d_model。\n",
    "pos决定了行，i决定了列。要构造两个矩阵pos和i，pos的每一行都是一样的，i的每一列都是一样的。用矩阵相乘的方式来构造PE。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0],\n",
       "         [1],\n",
       "         [2],\n",
       "         [3],\n",
       "         [4]]),\n",
       " tensor([[   1.,   10.,  100., 1000.]]))"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# position的最大长度\n",
    "max_position_len = 5 \n",
    "pos_mat = torch.arange(max_position_len).reshape((-1,1))\n",
    "i_mat = torch.pow(10000,torch.arange(0,max_num_src_words,2).reshape((1,-1))/model_dim)\n",
    "\n",
    "pos_mat,i_mat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
      "          1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
      "        [ 8.4147e-01,  5.4030e-01,  9.9833e-02,  9.9500e-01,  9.9998e-03,\n",
      "          9.9995e-01,  1.0000e-03,  1.0000e+00],\n",
      "        [ 9.0930e-01, -4.1615e-01,  1.9867e-01,  9.8007e-01,  1.9999e-02,\n",
      "          9.9980e-01,  2.0000e-03,  1.0000e+00],\n",
      "        [ 1.4112e-01, -9.8999e-01,  2.9552e-01,  9.5534e-01,  2.9995e-02,\n",
      "          9.9955e-01,  3.0000e-03,  1.0000e+00],\n",
      "        [-7.5680e-01, -6.5364e-01,  3.8942e-01,  9.2106e-01,  3.9989e-02,\n",
      "          9.9920e-01,  4.0000e-03,  9.9999e-01]])\n",
      "src_pos 为tensor([0, 1, 2, 3], dtype=torch.int32)\n",
      "tensor([[ 0.2615,  0.9311, -0.5145, -1.6517,  1.0460,  0.5222, -0.1668,  0.0530],\n",
      "        [ 0.5638,  2.2566,  1.8693, -1.1952,  0.9979,  0.4592,  2.4364, -0.1468],\n",
      "        [-0.4760, -0.2929, -0.3481,  0.3487,  0.0371, -0.0677,  0.4290, -0.8681],\n",
      "        [-0.2712,  0.1416,  0.1295,  0.6814, -0.9583,  0.0639,  0.6589,  0.8195]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "tensor([[ 6.6229e-01,  6.2291e-01,  4.4629e-01,  7.6881e-01,  1.5659e+00,\n",
      "          5.3203e-01, -1.4387e+00,  1.6551e+00],\n",
      "        [ 1.1658e+00,  5.8660e-01,  7.7713e-01,  4.0420e-01, -8.6360e-01,\n",
      "         -2.0782e-01,  6.7494e-01, -3.2825e-01],\n",
      "        [-1.1931e+00, -1.3558e-01,  2.8413e-01, -7.7306e-01,  4.4318e-04,\n",
      "         -5.7735e-01,  1.0048e+00, -9.4648e-01]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 定义pe_embedding_table，对奇偶数列进行赋值\n",
    "pe_embedding_table = torch.zeros(max_position_len,model_dim)\n",
    "pe_embedding_table[:,0::2]= torch.sin(pos_mat / i_mat)\n",
    "pe_embedding_table[:,1::2]= torch.cos(pos_mat / i_mat)\n",
    "\n",
    "pe_embedding = nn.Embedding(max_position_len, model_dim)\n",
    "pe_embedding.weight = nn.Parameter(pe_embedding_table, requires_grad=False)\n",
    "print(pe_embedding.weight)\n",
    "\n",
    "# 根据词的位置的索引来查表得到embedding,位置索引就是0,1,2,3,4，这里千万不要传入word_index\n",
    "src_pos = torch.Tensor(torch.arange(max(src_len))).to(torch.int32)\n",
    "tgt_pos = torch.Tensor(torch.arange(max(tgt_len))).to(torch.int32)\n",
    "print(f\"src_pos 为{src_pos}\")\n",
    "src_pe_embedding = src_embedding_table(src_pos)\n",
    "tgt_pe_embedding = tgt_embedding_table(tgt_pos)\n",
    "print(src_pe_embedding)\n",
    "print(tgt_pe_embedding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. mask\n",
    "mask的目的是为了让模型高效地训练好。一次训练会用多个样本，也就是mini-batch训练，由于序列长度不一样，就需要mask来保证得到的表征是有效的表征，也就是说不希望表征中有padding的那些符号的表征。\n",
    "\n",
    "$$Attention = \\frac{QK^T}{\\sqrt{d_k}}V$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0051, -1.6722,  0.8699, -2.2548, -0.4953])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5g/yvmtk8tn46v8qzbx5nl_4vd40000gn/T/ipykernel_7941/3969780852.py:17: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(score)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.1679, -0.0385, -0.0497, -0.0363, -0.0433],\n",
       "         [-0.0385,  0.1479, -0.0420, -0.0307, -0.0366],\n",
       "         [-0.0497, -0.0420,  0.1786, -0.0396, -0.0473],\n",
       "         [-0.0363, -0.0307, -0.0396,  0.1413, -0.0346],\n",
       "         [-0.0433, -0.0366, -0.0473, -0.0346,  0.1618]]),\n",
       " tensor([[ 1.7549e-04, -1.5994e-15, -1.7549e-04, -4.7188e-18, -2.0666e-10],\n",
       "         [-1.5994e-15,  9.1121e-12, -9.1105e-12, -2.4498e-25, -1.0729e-17],\n",
       "         [-1.7549e-04, -9.1105e-12,  1.7670e-04, -2.6880e-14, -1.1772e-06],\n",
       "         [-4.7188e-18, -2.4498e-25, -2.6880e-14,  2.6885e-14, -3.1654e-20],\n",
       "         [-2.0666e-10, -1.0729e-17, -1.1772e-06, -3.1654e-20,  1.1774e-06]]))"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scale的重要性演示，除以sqrt (dk)可以让分布不那么尖锐，方差更小\n",
    "# softmax, 下面的例子prob2的分布非常不平衡，prob1分布均匀\n",
    "score = torch.randn(5)\n",
    "print(score)\n",
    "alpha1 = 0.1\n",
    "alpha2= 10\n",
    "\n",
    "prob1 = F.softmax(score*alpha1, -1)\n",
    "prob2 = F.softmax(score*alpha2, -1)\n",
    "prob1,prob2\n",
    "#(tensor([0.2232, 0.1983, 0.1800, 0.2066, 0.1919]),\n",
    "# tensor([9.9954e-01, 7.3128e-06, 4.5446e-10, 4.5167e-04, 2.7027e-07]))#\n",
    "\n",
    "# Jakobian 矩阵（Jacobian 矩阵用于计算损失函数相对于网络参数的梯度）,下面例子中jacob_mat2接近0\n",
    "\n",
    "def softmax_func(score):\n",
    "    return F.softmax(score)\n",
    "\n",
    "jacob_mat1 = torch.autograd.functional.jacobian(softmax_func,score*alpha1)\n",
    "jacob_mat2 = torch.autograd.functional.jacobian(softmax_func,score*alpha2)\n",
    "jacob_mat1,jacob_mat2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 encoder self-attention mask\n",
    "序列自身对自身的关联性的计算，不涉及到因果，一次性输入src。\n",
    "\n",
    "mask放在softmax里面，既被mask的值希望是负无穷，那么softmax的概率接近于0，单词之间的关联性为0。\n",
    "mask矩阵元素值为1或者负无穷，shape为(batch_size,max_src_len, max_src_len) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 5])\n",
      "valid_encoder_pos_matrix:tensor([[[1., 1., 1., 0., 0.],\n",
      "         [1., 1., 1., 0., 0.],\n",
      "         [1., 1., 1., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 0.],\n",
      "         [1., 1., 1., 1., 0.],\n",
      "         [1., 1., 1., 1., 0.],\n",
      "         [1., 1., 1., 1., 0.],\n",
      "         [0., 0., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "# 构造有效编码器的位置，valid_encoder_pos的shape是(batch,1,T)\n",
    "valid_encoder_pos = torch.unsqueeze(torch.cat([torch.unsqueeze(F.pad(torch.ones(L),(0, max_src_seq_len-L)),0) for L in src_len ]), 2)\n",
    "valid_encoder_pos_matrix = torch.bmm(valid_encoder_pos, valid_encoder_pos.transpose(1,2))\n",
    "# src_seq的第一个的有效位置是前三个，第二个的有效位置是前四个\n",
    "print(valid_encoder_pos_matrix.shape)\n",
    "print(f\"valid_encoder_pos_matrix:{valid_encoder_pos_matrix}\")\n",
    "invalid_encoder_pos_matrix = 1- valid_encoder_pos_matrix\n",
    "mask_encoder_self_attention = invalid_encoder_pos_matrix.to(torch.bool)\n",
    "score = torch.randn(batch_size, max_src_seq_len,max_src_seq_len)\n",
    "# 对score进行mask\n",
    "masked_score = score.masked_fill(mask_encoder_self_attention, -1e9)\n",
    "prob = F.softmax(masked_score, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score:tensor([[[-0.6846,  0.0840, -0.9063, -1.4348,  0.6481],\n",
      "         [-0.2415, -0.0999,  1.0256, -0.1243, -0.6120],\n",
      "         [-1.2204,  1.6596,  0.8491,  0.0144,  0.9899],\n",
      "         [ 0.7697, -0.1433,  0.2896, -1.2278,  0.7073],\n",
      "         [ 0.5884,  0.1431, -0.9165, -0.4507, -0.6832]],\n",
      "\n",
      "        [[ 0.0307,  0.3432, -0.6461, -0.3701, -1.3320],\n",
      "         [ 0.6362, -0.3848, -0.2995, -0.2915, -1.0610],\n",
      "         [ 0.5787,  1.4887, -1.2425,  0.4335,  0.8034],\n",
      "         [ 0.8848, -0.5493, -0.6365, -1.6809, -0.6679],\n",
      "         [ 0.4959,  0.3084,  0.7170, -0.6831,  1.9426]]])\n",
      "masked_score:tensor([[[-6.8457e-01,  8.3978e-02, -9.0626e-01, -1.0000e+09, -1.0000e+09],\n",
      "         [-2.4148e-01, -9.9878e-02,  1.0256e+00, -1.0000e+09, -1.0000e+09],\n",
      "         [-1.2204e+00,  1.6596e+00,  8.4913e-01, -1.0000e+09, -1.0000e+09],\n",
      "         [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "         [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]],\n",
      "\n",
      "        [[ 3.0720e-02,  3.4323e-01, -6.4612e-01, -3.7011e-01, -1.0000e+09],\n",
      "         [ 6.3624e-01, -3.8484e-01, -2.9948e-01, -2.9147e-01, -1.0000e+09],\n",
      "         [ 5.7873e-01,  1.4887e+00, -1.2425e+00,  4.3352e-01, -1.0000e+09],\n",
      "         [ 8.8476e-01, -5.4928e-01, -6.3651e-01, -1.6809e+00, -1.0000e+09],\n",
      "         [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]]])\n",
      "prob:tensor([[[0.2527, 0.5449, 0.2024, 0.0000, 0.0000],\n",
      "         [0.1754, 0.2020, 0.6226, 0.0000, 0.0000],\n",
      "         [0.0374, 0.6663, 0.2963, 0.0000, 0.0000],\n",
      "         [0.2000, 0.2000, 0.2000, 0.2000, 0.2000],\n",
      "         [0.2000, 0.2000, 0.2000, 0.2000, 0.2000]],\n",
      "\n",
      "        [[0.2821, 0.3856, 0.1434, 0.1889, 0.0000],\n",
      "         [0.4656, 0.1677, 0.1826, 0.1841, 0.0000],\n",
      "         [0.2217, 0.5507, 0.0359, 0.1917, 0.0000],\n",
      "         [0.6520, 0.1554, 0.1424, 0.0501, 0.0000],\n",
      "         [0.2000, 0.2000, 0.2000, 0.2000, 0.2000]]])\n"
     ]
    }
   ],
   "source": [
    "print(f\"score:{score}\")\n",
    "print(f\"masked_score:{masked_score}\")\n",
    "print(f\"prob:{prob}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 intra-attention mask\n",
    "序列decoder seq对decoder seq的关联性的计算，涉及到因果，需要加mask。  \n",
    "Q @ K^T shape:[batch_size, tgt_seq_len, src_seq_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_decoder_pos:tensor([[[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]])\n",
      "valid_encoder_pos:tensor([[[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [0.]]])\n",
      "invalid_cross_pos_matrix:tensor([[[0., 0., 0., 1., 1.],\n",
      "         [0., 0., 0., 1., 1.],\n",
      "         [0., 0., 0., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 1.],\n",
      "         [0., 0., 0., 0., 1.],\n",
      "         [1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.]]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 构造有效解码器的位置\n",
    "valid_decoder_pos = torch.unsqueeze(torch.cat([torch.unsqueeze(F.pad(torch.ones(L),(0, max_tgt_seq_len-L)),0) for L in tgt_len ]), 2)\n",
    "# 目标序列对原序列的交叉矩阵\n",
    "valid_cross_pos_matrix = torch.bmm(valid_decoder_pos, valid_encoder_pos.transpose(1,2))\n",
    "print(f\"valid_decoder_pos:{valid_decoder_pos}\")\n",
    "print(f\"valid_encoder_pos:{valid_encoder_pos}\")\n",
    "invalid_cross_pos_matrix = 1- valid_cross_pos_matrix\n",
    "print(f\"invalid_cross_pos_matrix:{invalid_cross_pos_matrix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.2028, 0.3452, 0.4520, 0.0000, 0.0000],\n",
       "         [0.3709, 0.3258, 0.3033, 0.0000, 0.0000],\n",
       "         [0.6650, 0.1965, 0.1385, 0.0000, 0.0000],\n",
       "         [0.2000, 0.2000, 0.2000, 0.2000, 0.2000],\n",
       "         [0.2000, 0.2000, 0.2000, 0.2000, 0.2000]],\n",
       "\n",
       "        [[0.2563, 0.4074, 0.2614, 0.0749, 0.0000],\n",
       "         [0.2543, 0.3565, 0.2327, 0.1565, 0.0000],\n",
       "         [0.2000, 0.2000, 0.2000, 0.2000, 0.2000],\n",
       "         [0.2000, 0.2000, 0.2000, 0.2000, 0.2000],\n",
       "         [0.2000, 0.2000, 0.2000, 0.2000, 0.2000]]])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_decoder_cross_attention = invalid_cross_pos_matrix.to(torch.bool)\n",
    "score = torch.randn(batch_size, max_src_seq_len,max_src_seq_len)\n",
    "# 对score进行mask\n",
    "masked_score = score.masked_fill(mask_decoder_cross_attention, -1e9)\n",
    "prob = F.softmax(masked_score, -1)\n",
    "prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 decoder self-attention mask\n",
    "计算encoder的输出和mask-mha的输出的关联性\n",
    "这里是自回归，每个单词都在上一个单词预测的基础上。\n",
    "mask是一个三角形的矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]]), tensor([[1., 0.],\n",
      "        [1., 1.]])]\n",
      "tensor([[[1., 0., 0., 0., 0.],\n",
      "         [1., 1., 0., 0., 0.],\n",
      "         [1., 1., 1., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 0., 0., 0., 0.],\n",
      "         [1., 1., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.]]])\n",
      "torch.Size([2, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "# 对每个序列构建下三角，tgt第一个序列3*3，第二个序列2*2\n",
    "tri_mat = [torch.tril(torch.ones(L,L)) for L in tgt_len]\n",
    "print(tri_mat)\n",
    "\n",
    "# 构建有效decoder tri matrix，进行padding, concat\n",
    "valid_decoder_tri_matrix = torch.cat([torch.unsqueeze(F.pad(torch.tril(torch.ones((L,L))),\n",
    "                            (0,max_tgt_seq_len-L,0,max_tgt_seq_len-L)),0) for L in tgt_len])\n",
    "\n",
    "\n",
    "print(valid_decoder_tri_matrix)  \n",
    "print(valid_decoder_tri_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[False,  True,  True,  True,  True],\n",
      "         [False, False,  True,  True,  True],\n",
      "         [False, False, False,  True,  True],\n",
      "         [ True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[False,  True,  True,  True,  True],\n",
      "         [False, False,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.3884, 0.6116, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1540, 0.5055, 0.3405, 0.0000, 0.0000],\n",
       "         [0.2000, 0.2000, 0.2000, 0.2000, 0.2000],\n",
       "         [0.2000, 0.2000, 0.2000, 0.2000, 0.2000]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.8679, 0.1321, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2000, 0.2000, 0.2000, 0.2000, 0.2000],\n",
       "         [0.2000, 0.2000, 0.2000, 0.2000, 0.2000],\n",
       "         [0.2000, 0.2000, 0.2000, 0.2000, 0.2000]]])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "invalid_decoder_tri_matrix = 1- valid_decoder_tri_matrix\n",
    "invalid_decoder_tri_matrix = invalid_decoder_tri_matrix.to(torch.bool)\n",
    "print(invalid_decoder_tri_matrix)\n",
    "score = torch.randn(batch_size,max_tgt_seq_len,max_tgt_seq_len)\n",
    "masked_score = score.masked_fill(invalid_decoder_tri_matrix, -1e9)\n",
    "prob = F.softmax(masked_score, -1)\n",
    "prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. multi-head attention\n",
    "\n",
    "multi-head可以理解为batch，样本与样本之间独立，然后汇总去计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaled self attention\n",
    "# Q和K都是(batch_sieze*num_head, seq+len, model_dim/num_head)\n",
    "def scaled_dot_product_attention(Q,K,V,attn_mask):\n",
    "    score = torch.bmm(Q,K.transpose(-2,-1))/torch.sqrt(model_dim)\n",
    "    masked_score = score.masked_fill(attn_mask, -1e9)\n",
    "    prob = F.softmax(masked_score, -1)\n",
    "    context = torch.bmm(prob,V)\n",
    "    return context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Masked loss计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "logits = torch.randn(2,3,4)\n",
    "# batchsize=2, seqlen=3, vocab_size =4\n",
    "label = torch.randint(0,4,(2,3))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
