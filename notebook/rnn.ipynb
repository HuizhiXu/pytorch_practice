{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 单向RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.3019,  0.5374,  0.2303],\n",
      "         [ 0.4626,  0.2900,  0.5116],\n",
      "         [ 0.5831,  0.5606, -0.3392]],\n",
      "\n",
      "        [[ 0.5113,  0.4710, -0.3809],\n",
      "         [ 0.1795,  0.5527,  0.5527],\n",
      "         [-0.1945,  0.8211,  0.8665]]], grad_fn=<TransposeBackward1>)\n",
      "tensor([[[ 0.5831,  0.5606, -0.3392],\n",
      "         [-0.1945,  0.8211,  0.8665]]], grad_fn=<StackBackward0>)\n",
      "torch.Size([2, 3]) torch.Size([3]) torch.Size([2, 3]) torch.Size([3])\n",
      "torch.Size([2, 3]) torch.Size([3]) torch.Size([2, 3]) torch.Size([3])\n",
      "torch.Size([2, 3]) torch.Size([3]) torch.Size([2, 3]) torch.Size([3])\n",
      "tensor([[[ 0.3019,  0.5374,  0.2303],\n",
      "         [ 0.4626,  0.2900,  0.5116],\n",
      "         [ 0.5831,  0.5606, -0.3392]],\n",
      "\n",
      "        [[ 0.5113,  0.4710, -0.3809],\n",
      "         [ 0.1795,  0.5527,  0.5527],\n",
      "         [-0.1945,  0.8211,  0.8665]]], grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "batch_size, seq_len = 2,3\n",
    "input_size, hidden_size = 2,3 # 输入特征大小，隐含层特征大小\n",
    "\n",
    "input = torch.randn(batch_size,seq_len,input_size) # batch_size *seq_l*feature_size\n",
    "h_prev = torch.zeros(batch_size, hidden_size) # 初始隐含状态\n",
    "\"\"\"\n",
    " h_prev 的形状为 (3, 3)，表示：\n",
    "# [\n",
    "#   [0, 0, 0],  # 第一个样本的初始隐藏状态\n",
    "#   [0, 0, 0],  # 第二个样本的初始隐藏状态\n",
    "#   [0, 0, 0]  # 第三个样本的初始隐藏状态\n",
    "# ]\n",
    "\"\"\"\n",
    "\n",
    "# Step 1 调用PyTorch RNN API\n",
    "single_rnn = nn.RNN(input_size,hidden_size,batch_first = True) # 实例化\n",
    "output,hn = single_rnn(input, h_prev.unsqueeze(0))\n",
    "print(output)\n",
    "print(hn)\n",
    "\n",
    "# Step 2 手写一个rnn_forward函数，实现RNN的计算原理\n",
    "def rnn_forward(input, weight_ih, weight_hh, bias_ih,bias_hh,h_prev):\n",
    "  \n",
    "    batch_size, seq_len, input_size = input.shape\n",
    "    hidden_size = weight_ih.shape[0]\n",
    "    h_out = torch.zeros(batch_size,seq_len, hidden_size)\n",
    "\n",
    "    # 递归运算\n",
    "    for t in range(seq_len):\n",
    "\n",
    "        x = input[:,t,:].unsqueeze(2) # 获取当前维度的输入,batch_size * input_size \n",
    "        weight_ih_batch = weight_ih.unsqueeze(0).tile(batch_size, 1,1 )  # batch_size* hidden_size* input_size\n",
    "        # weight_ih形状通常是 (hidden_size, input_size)，需要转为batch\n",
    "        # weight_ih.unsqueeze(0)会将 weight_ih 的形状从 (hidden_size, input_size) 变为 (1, hidden_size, input_size)\n",
    "        # tile(batch_size, 1, 1)沿着第 0 维（batch 维度）重复 batch_size 次。沿着第 1 维（隐藏层大小维度）重复 1 次。沿着第 2 维（输入特征大小维度）重复 1 次。\n",
    "        weight_hh_batch = weight_hh.unsqueeze(0).tile(batch_size, 1,1 )   # batch_size* hidden_size* hidden_size\n",
    "\n",
    "        w_times_x= torch.bmm(weight_ih_batch, x).squeeze(-1) #  batch_size* hidden_size\n",
    "        w_times_h = torch.bmm(weight_hh_batch, h_prev.unsqueeze(2)).squeeze(-1) #  batch_size* hidden_size\n",
    "\n",
    "        # print(w_times_x.shape, bias_ih.shape, w_times_h.shape, bias_hh.shape)\n",
    "        h_prev = torch.tanh(w_times_x + bias_ih + w_times_h + bias_hh)\n",
    "\n",
    "        h_out[:,t,:] = h_prev\n",
    "\n",
    "    return h_out, h_prev.unsqueeze(0)\n",
    "\n",
    "# 验证rnn_forward的准确性：比较output和custom_rnn_out\n",
    "# for p,name in single_rnn.named_parameters():\n",
    "#     print(p,name)\n",
    "\n",
    "\n",
    "custom_rnn_out, custom_final_state = rnn_forward(input, \n",
    "                                                 single_rnn.weight_ih_l0, single_rnn.weight_hh_l0, \n",
    "                                                 single_rnn.bias_ih_l0, single_rnn.bias_hh_l0,\n",
    "                                                 h_prev)\n",
    "\n",
    "print(custom_rnn_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 双向RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "bidirectional_rnn_forward() missing 4 required positional arguments: 'weight_ih_reverse', 'weight_hh_reverse', 'bias_ih_reverse', and 'bias_hh_reverse'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 30\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m h_out, h_out[:,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,:]\u001b[38;5;241m.\u001b[39mreshape([batch_size,\u001b[38;5;241m2\u001b[39m,hidden_size])\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# 验证rnn_forward的准确性：比较output和custom_rnn_out\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# for p,name in single_rnn.named_parameters():\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m#     print(p,name)\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m custom_bi_rnn_out, custom_bi_final_state \u001b[38;5;241m=\u001b[39m \u001b[43mbidirectional_rnn_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m                                                 \u001b[49m\u001b[43msingle_rnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight_ih_l0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msingle_rnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight_hh_l0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m                                                 \u001b[49m\u001b[43msingle_rnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_ih_l0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msingle_rnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_hh_l0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m                                                 \u001b[49m\u001b[43mh_prev\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(custom_bi_rnn_out)\n",
      "\u001b[0;31mTypeError\u001b[0m: bidirectional_rnn_forward() missing 4 required positional arguments: 'weight_ih_reverse', 'weight_hh_reverse', 'bias_ih_reverse', and 'bias_hh_reverse'"
     ]
    }
   ],
   "source": [
    "bidirectional_rnn = nn.RNN(input_size,hidden_size,batch_first = True) # 实例化\n",
    "\n",
    "# 验证bi_rnn_forward的准确性：比较output和custom_rnn_out\n",
    "# for p,name in single_rnn.named_parameters():\n",
    "#     print(p,name)\n",
    "\n",
    "\n",
    "\n",
    "# Step 3 手写一个bidirectional_rnn_forward函数，实现双向RNN的计算原理\n",
    "def bidirectional_rnn_forward(input, weight_ih, weight_hh, bias_ih,bias_hh,h_prev,\n",
    "                              weight_ih_reverse,weight_hh_reverse, bias_ih_reverse,bias_hh_reverse):\n",
    "    # 所有的参数都是两份的\n",
    "  \n",
    "    batch_size, seq_len, input_size = input.shape\n",
    "    hidden_size = weight_ih.shape[0]\n",
    "    h_out = torch.zeros(batch_size,seq_len, hidden_size*2) # 双向 hidden_size变成两倍\n",
    "\n",
    "    \n",
    "     # 调用两遍rnn_forward\n",
    "    forward_output, _ = rnn_forward(input, weight_ih, weight_hh, bias_ih,bias_hh,h_prev) # forward layer \n",
    "    # backward_layer所有的权重都要用reverse版本，同时对于input 也要reverse一下，因为要保证第一个位置的input是最后一个元素\n",
    "    input_reverse = torch.flip(input, [1]) # [1]表示中间那一维，也就是在seq_len商\n",
    "    backward_output,_ =  rnn_forward(input_reverse, weight_ih_reverse, weight_hh_reverse, bias_ih_reverse,bias_hh_reverse,h_prev) # backward layer \n",
    "\n",
    "    \n",
    "\n",
    "    h_out[:,:,:hidden_size] = forward_output\n",
    "    h_out[:,:,hidden_size:] = backward_output\n",
    "\n",
    "    # h_prev可以取出h_out的最后一个时刻\n",
    "    return h_out, h_out[:,-1,:].reshape([batch_size,2,hidden_size]).transpose(0,1)\n",
    "\n",
    "\n",
    "\n",
    "custom_bi_rnn_out, custom_bi_final_state = bidirectional_rnn_forward(input, \n",
    "                                                 single_rnn.weight_ih_l0, single_rnn.weight_hh_l0, \n",
    "                                                 single_rnn.bias_ih_l0, single_rnn.bias_hh_l0,\n",
    "                                                 h_prev,\n",
    "                                                 )\n",
    "\n",
    "print(custom_bi_rnn_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
