{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 比较LSTM和GRU的参数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 150\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "lstm_layer = nn.LSTM(3,5) # input_size, hidden_size\n",
    "gru_layer = nn.GRU(3,5)\n",
    "\n",
    "num_lstm_p = sum(p.numel() for p in lstm_layer.parameters())\n",
    "num_gru_p = sum(p.numel() for p in gru_layer.parameters())\n",
    "\n",
    "print(num_lstm_p, num_gru_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 官方API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.3980, -1.8681,  0.4913, -0.5419,  0.0948],\n",
      "         [ 0.6235, -1.3081,  0.6259, -0.3296, -0.4504],\n",
      "         [ 0.2099, -0.9961,  0.7369, -0.3072, -0.1944]],\n",
      "\n",
      "        [[ 0.8470, -0.4014, -0.9545,  0.8089, -0.5204],\n",
      "         [ 0.2678, -0.0425, -0.7397,  0.7672, -0.1327],\n",
      "         [ 0.0273,  0.0763, -0.7891,  0.8487,  0.0870]]],\n",
      "       grad_fn=<TransposeBackward1>)\n",
      "weight_ih_l0 torch.Size([15, 4])\n",
      "weight_hh_l0 torch.Size([15, 5])\n",
      "bias_ih_l0 torch.Size([15])\n",
      "bias_hh_l0 torch.Size([15])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# input_size 输入序列的特征大小, seq_len：时间  hidden_size 网络的细胞状态的大小\n",
    "batch_size, seq_len, input_size,hidden_size = 2,3,4,5\n",
    "input = torch.randn(batch_size, seq_len, input_size)\n",
    "h0 = torch.randn(batch_size,hidden_size) \n",
    "\n",
    "gru_layer = nn.GRU(input_size,hidden_size, batch_first=True)\n",
    "output, h_final =gru_layer(input, h0.unsqueeze(0))\n",
    "print(output)\n",
    "\n",
    "\n",
    "for p,name in gru_layer.named_parameters():\n",
    "    print(p,name.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU的代码实现\n",
    "\n",
    "\\* 这个符号表示逐元素的相乘，Wx是矩阵相乘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3980, -1.8681,  0.4913, -0.5419,  0.0948],\n",
       "         [ 0.6235, -1.3081,  0.6259, -0.3296, -0.4504],\n",
       "         [ 0.2099, -0.9961,  0.7369, -0.3072, -0.1944]],\n",
       "\n",
       "        [[ 0.8470, -0.4014, -0.9545,  0.8089, -0.5204],\n",
       "         [ 0.2678, -0.0425, -0.7397,  0.7672, -0.1327],\n",
       "         [ 0.0273,  0.0763, -0.7891,  0.8487,  0.0870]]], grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def gru_forward(input, initial_states, w_ih, w_hh, b_ih, b_hh):\n",
    "\n",
    "    batch_size, seq_len, input_size = input.shape\n",
    "    prev_h = initial_states\n",
    "    hidden_size = w_ih.shape[0]//3  # 公式中只有三组相乘\n",
    "\n",
    "    # 对权重扩维，复制成batch_size倍\n",
    "    batch_w_ih = w_ih.unsqueeze(0).tile(batch_size, 1,1)\n",
    "    batch_w_hh = w_hh.unsqueeze(0).tile(batch_size, 1,1)\n",
    "\n",
    "    output = torch.zeros(batch_size, seq_len, hidden_size)\n",
    "\n",
    "\n",
    "    for t in range(seq_len):\n",
    "\n",
    "        x = input[:,t,:] # t时刻gru cell的输入特征向量  [batch_size, input_size]\n",
    "\n",
    "        w_times_x = torch.bmm(batch_w_ih, x.unsqueeze(-1)) # (batch_size, 3*hidden_size, 1)\n",
    "        w_times_x = w_times_x.squeeze(-1) # (batch_size, 3*hidden_size)\n",
    "\n",
    "        w_times_h_prev = torch.bmm(batch_w_hh, prev_h.unsqueeze(-1)) \n",
    "        w_times_h_prev = w_times_h_prev.squeeze(-1)   #  (batch_size, 3*hidden_size)\n",
    "\n",
    "        # 计算r_t和z_t  重置门和更新们\n",
    "\n",
    "        r_t = torch.sigmoid(w_times_x[:,:hidden_size] + w_times_h_prev[:,:hidden_size] +b_ih[:hidden_size] + b_hh[:hidden_size])\n",
    "        z_t = torch.sigmoid(w_times_x[:,hidden_size:hidden_size*2] + w_times_h_prev[:,hidden_size:hidden_size*2]  +b_ih[hidden_size:hidden_size*2] + b_hh[hidden_size:hidden_size*2])\n",
    "\n",
    "        # 计算候选状态 n_t\n",
    "\n",
    "        n_t = torch.tanh(w_times_x[:,hidden_size*2:hidden_size*3] + b_ih[2*hidden_size:3*hidden_size] \\\n",
    "                         + r_t*(w_times_h_prev[:,2*hidden_size:hidden_size*3] + b_hh[2*hidden_size:3*hidden_size])\n",
    "                         )\n",
    "        \n",
    "        prev_h = (1-z_t)*n_t + z_t*prev_h\n",
    "\n",
    "        output[:,t,:] = prev_h\n",
    "\n",
    "\n",
    "    return output,prev_h\n",
    "\n",
    "\n",
    "output_custom, h_final_custom = gru_forward(input, h0, gru_layer.weight_ih_l0, gru_layer.weight_hh_l0, gru_layer.bias_ih_l0, gru_layer.bias_hh_l0)    \n",
    "\n",
    "\n",
    "output_custom\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.allclose(output, output_custom))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
